train:
  max_steps: 50000

  # clip the gradient to have l2 norm at most this value
  gradient_clip_val: 10.0

  # whether to wrap the optimizer with online to nonconvex conversion
  # for some most optimizers/online learners, they have default value of wrap_o2nc 
  # (e.g., some online learners are always wrapped, and some optimizers are never wrapped),
  # which overwrites this setting.
  wrap_o2nc: False

  # random scaling options. supports "exponential".
  random_scaling: null
  random_scaling_seed: 0  # to be deprecated. we should only use one global random seed and generate sub-keys by jr.split()

  # whether to use automatic mixed precision
  use_amp: True
  # value to cast to in mixed precision training.
  precision: float16

  # whether to use volumization on the optimizer
  use_volumization: True

  volumization_opts:
    params_to_volumize: null
    eps: 1e-8
    stay_on_sphere: false
